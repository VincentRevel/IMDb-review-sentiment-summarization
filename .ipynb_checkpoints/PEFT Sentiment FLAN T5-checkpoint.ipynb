{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac40c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd87dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Set the default device for tensors\n",
    "torch.cuda.set_device(0)  # Set the GPU you want to use if you have multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f765f4",
   "metadata": {},
   "source": [
    "## 1 - Dataset and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999aaa0d",
   "metadata": {},
   "source": [
    "## 1.1 - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e73e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b115d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bf78e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b27b4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    start_prompt = 'Give sentiment positive or negative for the following movie review.\\n\\n'\n",
    "    end_prompt = '\\n\\nSentiment:: '\n",
    "\n",
    "    # Convert labels to text\n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    labels = []\n",
    "    for label in examples[\"label\"]:\n",
    "        if label in label_map:\n",
    "            labels.append(label_map[label])\n",
    "        else:\n",
    "            labels.append(\"unknown\")\n",
    "\n",
    "    # Construct prompts and tokenize\n",
    "    prompts = [start_prompt + text + end_prompt for text in examples[\"text\"]]\n",
    "    tokenized_inputs = original_tokenizer(prompts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_labels = original_tokenizer(labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Return the processed batch\n",
    "    return {\"input_ids\": tokenized_inputs.input_ids, \"labels\": tokenized_labels.input_ids, \"labeled\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7cb718e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b30626e98de47ce88a5b33f700ae46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a69df77dfe84d48bc80cdf1a447e596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222e687f30904985bd91f259da613941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82a493f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173ccc51f53545faa603895e82247cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc374dc43ca544a8a64ffd58cdfc1f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5e2106b6dd451487a34c769462d9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter every 100th example\n",
    "filtered_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68057d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (250, 5)\n",
      "Test: (250, 5)\n",
      "Unsupervised: (500, 5)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {filtered_datasets['train'].shape}\")\n",
    "print(f\"Test: {filtered_datasets['test'].shape}\")\n",
    "print(f\"Unsupervised: {filtered_datasets['unsupervised'].shape}\")\n",
    "\n",
    "print(filtered_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311d6fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1703ed78f104259a525804307911b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f3806cba6e4b0d8052bd96743da501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9826102fcfcc40da9190c1f0a45c8eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_filtered_datasets = dataset.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a81894ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (250, 2)\n",
      "Test: (250, 2)\n",
      "Unsupervised: (500, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {filtered_datasets['train'].shape}\")\n",
    "print(f\"Test: {filtered_datasets['test'].shape}\")\n",
    "print(f\"Unsupervised: {filtered_datasets['unsupervised'].shape}\")\n",
    "\n",
    "print(filtered_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa06b6",
   "metadata": {},
   "source": [
    "## 1.2 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41676aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Give sentiment positive or negative for the following movie review.\n",
      "\n",
      "He who fights with monsters might take care lest he thereby become a monster. And if you gaze for long into an abyss, the abyss gazes also into you.<br /><br />Yes, this is from Nietzsche's Aphorism 146 from \"Beyond Good and Evil\". And that's what you find at the start of this movie.<br /><br />If you watch the whole movie, you will doubt if it was the message that the Ram Gopal Varma Production wanted to pass on. As the scenes crop up one by one, quite violent and at times puke-raking, the viewer is expected to forget the Nietzsche quote and think otherwise. That to deal with few people you need dedicated people like Sadhu Agashe who will have the licence to kill anyone, not just writing FIRs (something unworthy of the police to do, as we are made to believe).<br /><br />When TADA was repealed and the government wanted to pass newer and even more draconian laws, RGV's \"Satya\" did the required brain surgery without blood transfusion for the multiplex growing thinking urban crowd whose views matters in a democratic country like India. Within a year MCOCA was passed.<br /><br />When real life encounter specialist Daya Nayak 'became a monster on the path of fighting them' and was himself booked by MCOCA, \"Aab tak Chhappan\" was made to heed out \"false\" impression among the people about this. With it's \"you have to be a monster to save your nation\" approach.<br /><br />And people consumed it. No questions raised. Only praises and hopes that they get a Sadhu Agashe in their local police station who will solve all problems and hence let only milk and butter flow all over. Blood? You can ignore.<br /><br />Every time Israel attacks Palestine or Lebanon, we hear voices like \"India must also similarly attack Pakistan\". This movie is made for such psychopaths. If you don't give them this, they will probably die out of boredom and LSD and what not.<br /><br />Hence this game of the passion of hatred.\n",
      "\n",
      "Sentiment:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE IMDB SENTIMENT:\n",
      "negative\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "text = dataset['test']['text'][index]\n",
    "label = dataset['test']['label'][index]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Give sentiment positive or negative for the following movie review.\n",
    "\n",
    "{text}\n",
    "\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "inputs = original_tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "output = original_tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Convert the label to \"negative\" if 0, and \"positive\" if 1\n",
    "sentiment_label = \"positive\" if label == 1 else \"negative\"\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE IMDB SENTIMENT:\\n{sentiment_label}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ff45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the first 5 examples in the dataset and generate predictions\n",
    "for i in range(5):\n",
    "    review = dataset['test']['text'][i]\n",
    "    inputs = original_tokenizer(review, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    outputs = original_model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\n",
    "    prediction = original_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Post-process the prediction to extract sentiment (negative or positive)\n",
    "    sentiment = 'positive' if 'good' in prediction else 'negative'\n",
    "    print(f\"Review: {review}\\nSentiment Prediction: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078f458",
   "metadata": {},
   "source": [
    "# 2 - Perform Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837645b",
   "metadata": {},
   "source": [
    "## 2.1 Setup the PEFT/LoRA model for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1635c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0356b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2126a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc55165",
   "metadata": {},
   "source": [
    "## 2.2 Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d55510",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-review-sentiment-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    max_steps=150    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a201354a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 1:00:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>37.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>13.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-review-sentiment-checkpoint-local\\\\tokenizer_config.json',\n",
       " './peft-review-sentiment-checkpoint-local\\\\special_tokens_map.json',\n",
       " './peft-review-sentiment-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-review-sentiment-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "original_tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5f5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "peft_model = PeftModel.from_pretrained(original_model, \n",
    "                                       './peft-dialogue-summary-checkpoint-local/',                                       \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf75045d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c7373",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd76011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 0, 'right')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model_try = PeftModel.from_pretrained(base_model, './peft-dialogue-summary-checkpoint-local/')\n",
    "peft_model_try = peft_model_try.merge_and_unload()\n",
    "\n",
    "tokenizer_peft = AutoTokenizer.from_pretrained(\"./peft-dialogue-summary-checkpoint-local/\", trust_remote_code=True)\n",
    "\n",
    "tokenizer_peft.pad_token, tokenizer_peft.pad_token_id, tokenizer_peft.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667c639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_try.pad_token_id = tokenizer_peft.pad_token_id\n",
    "peft_model_try.config.pad_token_id = tokenizer_peft.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a75264",
   "metadata": {},
   "source": [
    "## 2.4 Model Comparation to the original FLAN T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14f84b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "classify sentiment: \n",
      "what was Bruce Willis thinking when he signed on for this one?? this one made no sense to me.. i was so confused by it, it wasnt funny at all.. I dont even know why Disney made this one.. Bruce is a Great actor whom ive liked for a Long time .. and this disappointed me a lot.. Pass this one by on the video shelf....\n",
      "\n",
      "Sentiment:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE IMDB SENTIMENT:\n",
      "negative\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL GENERATION - ZERO SHOT:\n",
      "negative\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL GENERATION - ZERO SHOT:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "index = 150\n",
    "\n",
    "text = dataset['test']['text'][index]\n",
    "label = dataset['test']['label'][index]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "classify sentiment: \n",
    "{text}\n",
    "\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = original_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_output = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_sentiment_output = original_tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "peft_output = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_sentiment_output = original_tokenizer.decode(peft_output[0], skip_special_tokens=True)\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "# output = tokenizer.decode(\n",
    "#     peft_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=200)[0],\n",
    "#     skip_special_tokens=True\n",
    "# )\n",
    "\n",
    "# Convert the label to \"negative\" if 0, and \"positive\" if 1\n",
    "sentiment_label = \"positive\" if label == 1 else \"negative\"\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE IMDB SENTIMENT:\\n{sentiment_label}\\n')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL GENERATION - ZERO SHOT:\\n{original_model_sentiment_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL GENERATION - ZERO SHOT:\\n{peft_model_sentiment_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d2da8",
   "metadata": {},
   "source": [
    "## 2.4 Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01cee8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text2text-generation\", model=peft_model_try, tokenizer=tokenizer_peft, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f48e21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(input_text):\n",
    "    prompt = \"sentiment analysis: \" + input_text + \"\\n\\nSentiment:\"\n",
    "    output = pipe(prompt, max_length=16)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b898e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 6/250 [00:01<00:38,  6.36it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Evaluating: 100%|██████████| 250/250 [00:31<00:00,  7.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate over the test dataset and calculate predictions\n",
    "predictions = []\n",
    "for example in tqdm(filtered_datasets['test'], desc=\"Evaluating\"):\n",
    "    input_text = original_tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "    prediction = calculate_sentiment(input_text)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7de9bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "Precision: 0.9402535860655737\n",
      "Recall: 0.94\n",
      "F1 Score: 0.9399913587556609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming the labels are also available in the test dataset, calculate accuracy\n",
    "correct_predictions = sum(1 for i, example in enumerate(filtered_datasets['test']) if predictions[i] == example['labeled'])\n",
    "total_predictions = len(predictions)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "y_true = [example['labeled'] for example in filtered_datasets['test']]\n",
    "precision = precision_score(y_true, predictions, average='weighted')\n",
    "recall = recall_score(y_true, predictions, average='weighted')\n",
    "f1 = f1_score(y_true, predictions, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "480b3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[119   6]\n",
      " [  9 116]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8c9c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94       125\n",
      "    positive       0.95      0.93      0.94       125\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.94      0.94      0.94       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9891ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9399999999999998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert labels to binary (1 for positive, 0 for negative)\n",
    "predictions_numeric = [1 if label == 'positive' else 0 for label in predictions]\n",
    "\n",
    "y_true_binary = filtered_datasets['test']['label']\n",
    "\n",
    "roc_auc = roc_auc_score(y_true_binary, predictions_numeric)\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
