{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac40c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd87dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Set the default device for tensors\n",
    "torch.cuda.set_device(0)  # Set the GPU you want to use if you have multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f765f4",
   "metadata": {},
   "source": [
    "## 1 - Dataset and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999aaa0d",
   "metadata": {},
   "source": [
    "## 1.1 - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e73e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b115d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bf78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27b4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    start_prompt = 'Give sentiment positive or negative for the following movie review.\\n\\n'\n",
    "    end_prompt = '\\n\\nSentiment:: '\n",
    "\n",
    "    # Convert labels to text\n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    labels = []\n",
    "    for label in examples[\"label\"]:\n",
    "        if label in label_map:\n",
    "            labels.append(label_map[label])\n",
    "        else:\n",
    "            labels.append(\"unknown\")\n",
    "\n",
    "    # Construct prompts and tokenize\n",
    "    prompts = [start_prompt + text + end_prompt for text in examples[\"text\"]]\n",
    "    tokenized_inputs = original_tokenizer(prompts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_labels = original_tokenizer(labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Return the processed batch\n",
    "    return {\"input_ids\": tokenized_inputs.input_ids, \"labels\": tokenized_labels.input_ids, \"labeled\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cb718e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5d8a1115c94cf7af2ddccf707d0288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a493f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47329df1409e4f1ea9c48b73508ca512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter every 100th example\n",
    "filtered_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68057d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (250, 5)\n",
      "Test: (250, 5)\n",
      "Unsupervised: (500, 5)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {filtered_datasets['train'].shape}\")\n",
    "print(f\"Test: {filtered_datasets['test'].shape}\")\n",
    "print(f\"Unsupervised: {filtered_datasets['unsupervised'].shape}\")\n",
    "\n",
    "print(filtered_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "311d6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_filtered_datasets = dataset.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a81894ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (250, 5)\n",
      "Test: (250, 5)\n",
      "Unsupervised: (500, 5)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'labels', 'labeled'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {filtered_datasets['train'].shape}\")\n",
    "print(f\"Test: {filtered_datasets['test'].shape}\")\n",
    "print(f\"Unsupervised: {filtered_datasets['unsupervised'].shape}\")\n",
    "\n",
    "print(filtered_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa06b6",
   "metadata": {},
   "source": [
    "## 1.2 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41676aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Give sentiment positive or negative for the following movie review.\n",
      "\n",
      "He who fights with monsters might take care lest he thereby become a monster. And if you gaze for long into an abyss, the abyss gazes also into you.<br /><br />Yes, this is from Nietzsche's Aphorism 146 from \"Beyond Good and Evil\". And that's what you find at the start of this movie.<br /><br />If you watch the whole movie, you will doubt if it was the message that the Ram Gopal Varma Production wanted to pass on. As the scenes crop up one by one, quite violent and at times puke-raking, the viewer is expected to forget the Nietzsche quote and think otherwise. That to deal with few people you need dedicated people like Sadhu Agashe who will have the licence to kill anyone, not just writing FIRs (something unworthy of the police to do, as we are made to believe).<br /><br />When TADA was repealed and the government wanted to pass newer and even more draconian laws, RGV's \"Satya\" did the required brain surgery without blood transfusion for the multiplex growing thinking urban crowd whose views matters in a democratic country like India. Within a year MCOCA was passed.<br /><br />When real life encounter specialist Daya Nayak 'became a monster on the path of fighting them' and was himself booked by MCOCA, \"Aab tak Chhappan\" was made to heed out \"false\" impression among the people about this. With it's \"you have to be a monster to save your nation\" approach.<br /><br />And people consumed it. No questions raised. Only praises and hopes that they get a Sadhu Agashe in their local police station who will solve all problems and hence let only milk and butter flow all over. Blood? You can ignore.<br /><br />Every time Israel attacks Palestine or Lebanon, we hear voices like \"India must also similarly attack Pakistan\". This movie is made for such psychopaths. If you don't give them this, they will probably die out of boredom and LSD and what not.<br /><br />Hence this game of the passion of hatred.\n",
      "\n",
      "Sentiment:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE IMDB SENTIMENT:\n",
      "negative\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "text = dataset['test']['text'][index]\n",
    "label = dataset['test']['label'][index]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Give sentiment positive or negative for the following movie review.\n",
    "\n",
    "{text}\n",
    "\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "inputs = original_tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "output = original_tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Convert the label to \"negative\" if 0, and \"positive\" if 1\n",
    "sentiment_label = \"positive\" if label == 1 else \"negative\"\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE IMDB SENTIMENT:\\n{sentiment_label}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3ff45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "Sentiment Prediction: positive\n",
      "\n",
      "Review: Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\n",
      "Sentiment Prediction: negative\n",
      "\n",
      "Review: its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n",
      "Sentiment Prediction: negative\n",
      "\n",
      "Review: STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\n",
      "Sentiment Prediction: negative\n",
      "\n",
      "Review: First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
      "Sentiment Prediction: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over the first 5 examples in the dataset and generate predictions\n",
    "for i in range(5):\n",
    "    review = dataset['test']['text'][i]\n",
    "    inputs = original_tokenizer(review, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    outputs = original_model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\n",
    "    prediction = original_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Post-process the prediction to extract sentiment (negative or positive)\n",
    "    sentiment = 'positive' if 'good' in prediction else 'negative'\n",
    "    print(f\"Review: {review}\\nSentiment Prediction: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078f458",
   "metadata": {},
   "source": [
    "# 2 - Perform Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837645b",
   "metadata": {},
   "source": [
    "## 2.1 Setup the PEFT/LoRA model for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1635c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0356b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2126a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc55165",
   "metadata": {},
   "source": [
    "## 2.2 Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d55510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-review-sentiment-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    max_steps=150    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a201354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 2:14:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>37.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>11.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\anaconda3\\envs\\imdb-env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft-review-sentiment-checkpoint-local\\\\tokenizer_config.json',\n",
       " './peft-review-sentiment-checkpoint-local\\\\special_tokens_map.json',\n",
       " './peft-review-sentiment-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-review-sentiment-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "original_tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc5f5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "peft_model = PeftModel.from_pretrained(original_model, \n",
    "                                       './peft-dialogue-summary-checkpoint-local/',                                       \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf75045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c7373",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbd76011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 0, 'right')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model_try = PeftModel.from_pretrained(base_model, './peft-dialogue-summary-checkpoint-local/')\n",
    "peft_model_try = peft_model_try.merge_and_unload()\n",
    "\n",
    "tokenizer_peft = AutoTokenizer.from_pretrained(\"./peft-dialogue-summary-checkpoint-local/\", trust_remote_code=True)\n",
    "\n",
    "tokenizer_peft.pad_token, tokenizer_peft.pad_token_id, tokenizer_peft.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "667c639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_try.pad_token_id = tokenizer_peft.pad_token_id\n",
    "peft_model_try.config.pad_token_id = tokenizer_peft.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a75264",
   "metadata": {},
   "source": [
    "## 2.4 Model Comparation to the original FLAN T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d7bd681-4e48-4402-afa0-8b77aeb7647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "classify sentiment: \n",
      "Blake Edwards' legendary fiasco, begins to seem pointless after just 10 minutes. A combination of The Eagle Has Landed, Star!, Oh! What a Lovely War!, and Edwards' Pink Panther films, Darling Lili never engages the viewer; the aerial sequences, the musical numbers, the romance, the comedy, and the espionage are all ho hum. At what point is the viewer supposed to give a damn? This disaster wavers in tone, never decides what it wants to be, and apparently thinks it's a spoof, but it's pathetically and grindingly square. Old fashioned in the worst sense, audiences understandably stayed away in droves. It's awful. James Garner would have been a vast improvement over Hudson who is just cardboard, and he doesn't connect with Andrews and vice versa. And both Andrews and Hudson don't seem to have been let in on the joke and perform with a miscalculated earnestness. Blake Edwards' SOB isn't much more than OK, but it's the only good that ever came out of Darling Lili. The expensive and professional look of much of Darling Lili, only make what it's all lavished on even more difficult to bear. To quote Paramount chief Robert Evans, \"24 million dollars worth of film and no picture\".\n",
      "\n",
      "Sentiment:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE IMDB SENTIMENT:\n",
      "negative\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL GENERATION - ZERO SHOT:\n",
      "negative\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL GENERATION - ZERO SHOT:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "index = 120\n",
    "\n",
    "text = dataset['test']['text'][index]\n",
    "label = dataset['test']['label'][index]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "classify sentiment: \n",
    "{text}\n",
    "\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Move models to the appropriate device\n",
    "original_model.to(device)\n",
    "peft_model.to(device)\n",
    "\n",
    "# Tokenize the input and move to the same device\n",
    "input_ids = original_tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "# Adjust generation parameters\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1,\n",
    "    num_beams=4,  # Increase the number of beams for better exploration\n",
    "    temperature=0.7,  # Add temperature to control randomness\n",
    "    top_k=50,  # Add top_k sampling\n",
    "    top_p=0.95  # Add top_p sampling\n",
    ")\n",
    "\n",
    "# Generate output with the original model\n",
    "original_output = original_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "original_model_sentiment_output = original_tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate output with the PEFT model\n",
    "peft_output = peft_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "peft_model_sentiment_output = original_tokenizer.decode(peft_output[0], skip_special_tokens=True)\n",
    "\n",
    "# Convert the label to \"negative\" if 0, and \"positive\" if 1\n",
    "sentiment_label = \"positive\" if label == 1 else \"negative\"\n",
    "\n",
    "dash_line = '-' * 100\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE IMDB SENTIMENT:\\n{sentiment_label}\\n')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL GENERATION - ZERO SHOT:\\n{original_model_sentiment_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL GENERATION - ZERO SHOT:\\n{peft_model_sentiment_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d2da8",
   "metadata": {},
   "source": [
    "## 2.4 Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01cee8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text2text-generation\", model=peft_model_try, tokenizer=tokenizer_peft, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f48e21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(input_text):\n",
    "    prompt = \"sentiment analysis: \" + input_text + \"\\n\\nSentiment:\"\n",
    "    output = pipe(prompt, max_length=16)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b898e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|█▋                                                                    | 6/250 [00:01<00:57,  4.26it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Evaluating:   4%|██▊                                                                  | 10/250 [00:02<00:54,  4.43it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 250/250 [01:05<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate over the test dataset and calculate predictions\n",
    "predictions = []\n",
    "for example in tqdm(filtered_datasets['test'], desc=\"Evaluating\"):\n",
    "    input_text = original_tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "    prediction = calculate_sentiment(input_text)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7de9bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "Precision: 0.9402535860655737\n",
      "Recall: 0.94\n",
      "F1 Score: 0.9399913587556609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming the labels are also available in the test dataset, calculate accuracy\n",
    "correct_predictions = sum(1 for i, example in enumerate(filtered_datasets['test']) if predictions[i] == example['labeled'])\n",
    "total_predictions = len(predictions)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "y_true = [example['labeled'] for example in filtered_datasets['test']]\n",
    "precision = precision_score(y_true, predictions, average='weighted')\n",
    "recall = recall_score(y_true, predictions, average='weighted')\n",
    "f1 = f1_score(y_true, predictions, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "480b3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[119   6]\n",
      " [  9 116]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8c9c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94       125\n",
      "    positive       0.95      0.93      0.94       125\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.94      0.94      0.94       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9891ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9399999999999998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert labels to binary (1 for positive, 0 for negative)\n",
    "predictions_numeric = [1 if label == 'positive' else 0 for label in predictions]\n",
    "\n",
    "y_true_binary = filtered_datasets['test']['label']\n",
    "\n",
    "roc_auc = roc_auc_score(y_true_binary, predictions_numeric)\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cf6d5-cac2-4db0-95ad-b37e8781f38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
